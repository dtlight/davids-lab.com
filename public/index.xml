<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>David&#39;s Lab on David&#39;s Home Lab</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in David&#39;s Lab on David&#39;s Home Lab</description>
    <generator>Hugo -- 0.152.2</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 14 Dec 2025 09:09:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Control Plane Setup</title>
      <link>http://localhost:1313/blogs/control-plane-setup/</link>
      <pubDate>Sun, 14 Dec 2025 09:09:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/control-plane-setup/</guid>
      <description>&lt;p&gt;This concerns setting up the control plane nodes which run a fresh installation of Pi OS. I&amp;rsquo;ve already reserved IPs for all Raspberry Pis on my router. For my homelab static IPs will ensure stable Kubernetes node communication, ExternalDNS updates, and Harbor registry access without lease expiration disruptions.&lt;/p&gt;
&lt;h2 id=&#34;create-staic-ips-for-cp-node--kube-vip&#34;&gt;Create staic IPs for CP node + kube vip:&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Set primary + VIP and gateway&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo nmcli connection modify &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Wired connection 1&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  ipv4.addresses &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;192.168.8.201/24,192.168.8.205/32&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  ipv4.gateway &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;192.168.8.1&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  ipv4.method manual
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Apply immediately&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo nmcli connection down &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Wired connection 1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo nmcli connection up &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Wired connection 1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;disable-swap-and-set-cgroup-drivers-for-kubelet-compatibility&#34;&gt;Disable swap and set cgroup drivers for kubelet compatibility.&lt;/h2&gt;
&lt;p&gt;Kubernetes requires swap disabled by default (NoSwap behavior) for predictable pod scheduling and to avoid OOM issues.
Disable swap and configure the kubelet cgroup driver to systemd on control plane nodes to ensure kubelet compatibility and prevent preflight errors during kubeadm init or kubeadm join&lt;/p&gt;</description>
    </item>
    <item>
      <title>Resource Planning</title>
      <link>http://localhost:1313/blogs/resource-planning/</link>
      <pubDate>Sat, 13 Dec 2025 09:09:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/resource-planning/</guid>
      <description>&lt;h2 id=&#34;resource-planning&#34;&gt;Resource Planning&lt;/h2&gt;
&lt;p&gt;My four node cluster currently consists of:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Two control plane nodes (cpl1 and cpl2 16gb ram): both running Raspberry Pi OS (Debian 13) in headless mode so are lightweight and Pi optimised. Etcd requires the most RAM (3GB for small clusters like mine with &amp;lt;100 pods), while other components should typically stay under 1GB each (with Prometheus at roughly 3gb as an exception) with my multi-node control plane set up which distributes load, so 16GB handles 20-50 pods across nodes without OOM issues.&lt;/li&gt;
&lt;li&gt;Two worker nodes (wn1, wn2), one runs Talos OS and the other Debian. Talos is a highly secure yet minimal operating system. It runs Kubernetes natively with no host OS services competing for resources (workers use roughly 200MB RAM vs 1GB+ on Debian). However Talos requires control of the entire disk space (that is, it wipes and repartitions the full disk with a fixed layout) so i am limiting it to 1 worker node as I want the option of using the other worker node (an old ThinkPad) if I need it.
I have mixed 8GB and 16GB worker nodes which work fine for my Kubernetes homelab, as schedulers automatically balance pods based on allocatable resources (node taints/labels also help). 8GB should handle 20-40 pods (e.g., Prometheus targets, Home Assistant), while 16GB can take heavier workloads like databases or future AI experiments without OOM issues.â€‹&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Having two control plane nodes promotes High Availability (H.A) for the things I will run on my cluster, which would be frustrating to have offline:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
